{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2U0NA9HrrZey"
   },
   "source": [
    "# Taming Transformers\n",
    "\n",
    "This notebook is a minimal working example to generate landscape images as in [Taming Transformers for High-Resolution Image Synthesis](https://github.com/CompVis/taming-transformers). **tl;dr** We combine the efficiancy of convolutional approaches with the expressivity of transformers by introducing a convolutional VQGAN, which learns a codebook of context-rich visual parts, whose composition is modeled with an autoregressive transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4hdc6YonrvoC"
   },
   "source": [
    "## Setup\n",
    "The setup code in this section was written to be [run in a Colab environment](https://colab.research.google.com/github/CompVis/taming-transformers/blob/master/scripts/taming-transformers.ipynb). For a full, local setup, we recommend the provided [conda environment](https://github.com/CompVis/taming-transformers/blob/master/environment.yaml), as [described in the readme](https://github.com/CompVis/taming-transformers#requirements). This will also allow you to run a streamlit based demo.\n",
    "\n",
    "Here, we first clone the repository and download a model checkpoint and config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wwj8j_l201aF",
    "outputId": "833f54a6-0620-4dc1-dd8c-69ab01433d17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/g/github/ZSE-SBIR\n",
      "['/mnt/g/github/ZSE-SBIR/taming-transformers/scripts', '/home/bruce/anaconda3/lib/python310.zip', '/home/bruce/anaconda3/lib/python3.10', '/home/bruce/anaconda3/lib/python3.10/lib-dynload', '', '/home/bruce/anaconda3/lib/python3.10/site-packages', '/home/bruce/anaconda3/lib/python3.10/site-packages/PyQt5_sip-12.11.0-py3.10-linux-x86_64.egg', '/home/bruce/anaconda3/lib/python3.10/site-packages/mpmath-1.2.1-py3.10.egg', 'taming', 'taming']\n"
     ]
    }
   ],
   "source": [
    "# !git clone https://github.com/CompVis/taming-transformers\n",
    "import os \n",
    "print(os.getcwd())\n",
    "import sys\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd taming-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir -p logs/2020-11-09T13-31-51_sflckr/checkpoints -p\n",
    "# !wget 'https://heibox.uni-heidelberg.de/d/73487ab6e5314cb5adba/files/?p=%2Fcheckpoints%2Flast.ckpt&dl=1' -O 'logs/2020-11-09T13-31-51_sflckr/checkpoints/last.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir logs/2020-11-09T13-31-51_sflckr/configs -p\n",
    "# !wget 'https://heibox.uni-heidelberg.de/d/73487ab6e5314cb5adba/files/?p=%2Fconfigs%2F2020-11-09T13-31-51-project.yaml&dl=1' -O 'logs/2020-11-09T13-31-51_sflckr/configs/2020-11-09T13-31-51-project.yaml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eeBqWgMQDjZb"
   },
   "source": [
    "Next, we install minimal required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hzQNmIuT_0uF",
    "outputId": "e6e1ce0f-bce9-4ebe-a852-95003ca7b630"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: 2.0.0 not found\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "['/mnt/g/github/ZSE-SBIR/taming-transformers/scripts', '/home/bruce/anaconda3/lib/python310.zip', '/home/bruce/anaconda3/lib/python3.10', '/home/bruce/anaconda3/lib/python3.10/lib-dynload', '', '/home/bruce/anaconda3/lib/python3.10/site-packages', '/home/bruce/anaconda3/lib/python3.10/site-packages/PyQt5_sip-12.11.0-py3.10-linux-x86_64.egg', '/home/bruce/anaconda3/lib/python3.10/site-packages/mpmath-1.2.1-py3.10.egg', 'taming', 'taming']\n"
     ]
    }
   ],
   "source": [
    "%pip install omegaconf>=2.0.0 pytorch-lightning>=1.0.8 einops transformers\n",
    "import sys\n",
    "sys.path.append(\"taming\")\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5gaAQZXTxFxD"
   },
   "source": [
    "## Loading the model\n",
    "\n",
    "We load and print the config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hUOUJaTj02Bq",
    "outputId": "6671562d-f7f6-4407-ee40-22e1b83421e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:\n",
      "  params:\n",
      "    batch_size: 1\n",
      "    validation:\n",
      "      target: taming.data.sflckr.Examples\n",
      "  target: main.DataModuleFromConfig\n",
      "model:\n",
      "  base_learning_rate: 4.5e-06\n",
      "  params:\n",
      "    cond_stage_config:\n",
      "      params:\n",
      "        ddconfig:\n",
      "          attn_resolutions:\n",
      "          - 16\n",
      "          ch: 128\n",
      "          ch_mult:\n",
      "          - 1\n",
      "          - 1\n",
      "          - 2\n",
      "          - 2\n",
      "          - 4\n",
      "          double_z: false\n",
      "          dropout: 0.0\n",
      "          in_channels: 182\n",
      "          num_res_blocks: 2\n",
      "          out_ch: 182\n",
      "          resolution: 256\n",
      "          z_channels: 256\n",
      "        embed_dim: 256\n",
      "        image_key: segmentation\n",
      "        lossconfig:\n",
      "          target: taming.modules.losses.DummyLoss\n",
      "        n_embed: 1024\n",
      "      target: taming.models.vqgan.VQModel\n",
      "    cond_stage_key: segmentation\n",
      "    first_stage_config:\n",
      "      params:\n",
      "        ddconfig:\n",
      "          attn_resolutions:\n",
      "          - 16\n",
      "          ch: 128\n",
      "          ch_mult:\n",
      "          - 1\n",
      "          - 1\n",
      "          - 2\n",
      "          - 2\n",
      "          - 4\n",
      "          double_z: false\n",
      "          dropout: 0.0\n",
      "          in_channels: 3\n",
      "          num_res_blocks: 2\n",
      "          out_ch: 3\n",
      "          resolution: 256\n",
      "          z_channels: 256\n",
      "        embed_dim: 256\n",
      "        lossconfig:\n",
      "          target: taming.modules.losses.DummyLoss\n",
      "        n_embed: 1024\n",
      "      target: taming.models.vqgan.VQModel\n",
      "    first_stage_key: image\n",
      "    transformer_config:\n",
      "      params:\n",
      "        block_size: 512\n",
      "        n_embd: 1024\n",
      "        n_head: 16\n",
      "        n_layer: 24\n",
      "        vocab_size: 1024\n",
      "      target: taming.modules.transformer.mingpt.GPT\n",
      "  target: taming.models.cond_transformer.Net2NetTransformer\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from omegaconf import OmegaConf\n",
    "config_path = \"logs/2020-11-09T13-31-51_sflckr/configs/2023-09-13-project.yaml\"\n",
    "\n",
    "config = OmegaConf.load(config_path)\n",
    "import yaml\n",
    "print(yaml.dump(OmegaConf.to_container(config)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/g/github/ZSE-SBIR/taming-transformers\n",
      "/mnt/g/github/ZSE-SBIR/taming-transformers\n"
     ]
    }
   ],
   "source": [
    "%cd /mnt/g/github/ZSE-SBIR/taming-transformers\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FzQqNgiLEJ9J"
   },
   "source": [
    "Instantiate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sWvDDVoz3RB4",
    "outputId": "08462e1d-77da-4f9d-d3e9-43bdd2be74b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with z of shape (1, 256, 16, 16) = 65536 dimensions.\n",
      "Working with z of shape (1, 256, 16, 16) = 65536 dimensions.\n"
     ]
    }
   ],
   "source": [
    "from taming.models.cond_transformer import Net2NetTransformer\n",
    "\n",
    "model = Net2NetTransformer(**config.model.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "njAiY_aqENwV"
   },
   "source": [
    "Load the checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "QABLRpVsDhba"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "ckpt_path = \"logs/2020-11-09T13-31-51_sflckr/checkpoints/last.ckpt\"\n",
    "sd = torch.load(ckpt_path, map_location=\"cpu\")[\"state_dict\"]\n",
    "missing, unexpected = model.load_state_dict(sd, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iYN9cREY-r3N",
    "outputId": "0954af07-f4b9-474a-d6bf-45355f2b360d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f863faa3550>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cuda().eval()\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tTusbqk2y0u3"
   },
   "source": [
    "## Load example data\n",
    "\n",
    "Load an example segmentation and visualize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "8LiAOU6C-vTP"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "segmentation_path = \"data/patch_append/patch_replace.jpg\"\n",
    "segmentation = Image.open(segmentation_path)\n",
    "segmentation = np.array(segmentation)\n",
    "\n",
    "segmentation = torch.tensor(segmentation.transpose(2,0,1)[None]).to(dtype=torch.float32, device=model.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oCNy13FGPMy6"
   },
   "source": [
    "Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 700
    },
    "id": "eZFxCkQ5McKG",
    "outputId": "367a3774-3fcc-405c-c643-4477aad91a2d"
   },
   "outputs": [],
   "source": [
    "def show_segmentation(s):\n",
    "  s = s.detach().cpu().numpy().transpose(0,2,3,1)[0,:,:,None,:]\n",
    "  colorize = np.random.RandomState(1).randn(1,1,s.shape[-1],3)\n",
    "  colorize = colorize / colorize.sum(axis=2, keepdims=True)\n",
    "  s = s@colorize\n",
    "  s = s[...,0,:]\n",
    "  s = ((s+1.0)*127.5).clip(0,255).astype(np.uint8)\n",
    "  s = Image.fromarray(s)\n",
    "  display(s)\n",
    "\n",
    "show_segmentation(segmentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hNBdHGbNTrfM"
   },
   "source": [
    "Our model also employs a VQGAN for the conditioning information, i.e. the segmentation in this example. Let's autoencode the segmentation map. Encoding returns both the quantized code and its representation in terms of indices of a learned codebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 724
    },
    "id": "y_HS2hCCORLr",
    "outputId": "770227a3-533e-4e77-a017-8f6a9b919940"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [128, 182, 3, 3], expected input[1, 3, 254, 254] to have 182 channels, but got 3 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m c_code, c_indices \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mencode_to_c(segmentation)\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mc_code\u001b[39m\u001b[39m\"\u001b[39m, c_code\u001b[39m.\u001b[39mshape, c_code\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mc_indices\u001b[39m\u001b[39m\"\u001b[39m, c_indices\u001b[39m.\u001b[39mshape, c_indices\u001b[39m.\u001b[39mdtype)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/mnt/g/github/ZSE-SBIR/taming-transformers/taming/models/cond_transformer.py:179\u001b[0m, in \u001b[0;36mNet2NetTransformer.encode_to_c\u001b[0;34m(self, c)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdownsample_cond_size \u001b[39m>\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n\u001b[1;32m    178\u001b[0m     c \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39minterpolate(c, size\u001b[39m=\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdownsample_cond_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdownsample_cond_size))\n\u001b[0;32m--> 179\u001b[0m quant_c, _, [_,_,indices] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcond_stage_model\u001b[39m.\u001b[39;49mencode(c)\n\u001b[1;32m    180\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(indices\u001b[39m.\u001b[39mshape) \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m    181\u001b[0m     indices \u001b[39m=\u001b[39m indices\u001b[39m.\u001b[39mview(c\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/mnt/g/github/ZSE-SBIR/taming-transformers/taming/models/vqgan.py:56\u001b[0m, in \u001b[0;36mVQModel.encode\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mencode\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 56\u001b[0m     h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(x)\n\u001b[1;32m     57\u001b[0m     h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquant_conv(h)\n\u001b[1;32m     58\u001b[0m     quant, emb_loss, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquantize(h)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/g/github/ZSE-SBIR/taming-transformers/taming/modules/diffusionmodules/model.py:413\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    410\u001b[0m temb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    412\u001b[0m \u001b[39m# downsampling\u001b[39;00m\n\u001b[0;32m--> 413\u001b[0m hs \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv_in(x)]\n\u001b[1;32m    414\u001b[0m \u001b[39mfor\u001b[39;00m i_level \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_resolutions):\n\u001b[1;32m    415\u001b[0m     \u001b[39mfor\u001b[39;00m i_block \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_res_blocks):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [128, 182, 3, 3], expected input[1, 3, 254, 254] to have 182 channels, but got 3 channels instead"
     ]
    }
   ],
   "source": [
    "c_code, c_indices = model.encode_to_c(segmentation)\n",
    "print(\"c_code\", c_code.shape, c_code.dtype)\n",
    "print(\"c_indices\", c_indices.shape, c_indices.dtype)\n",
    "assert c_code.shape[2]*c_code.shape[3] == c_indices.shape[0]\n",
    "segmentation_rec = model.cond_stage_model.decode(c_code)\n",
    "show_segmentation(torch.softmax(segmentation_rec, dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pp-OsC1RXAQ9"
   },
   "source": [
    "Let's sample indices corresponding to codes from the image VQGAN given the segmentation code. We init randomly and take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 689
    },
    "id": "VTfao3jJSCfW",
    "outputId": "1a3320ff-a389-4759-ced3-5dee58bf7a29"
   },
   "outputs": [],
   "source": [
    "def show_image(s):\n",
    "  s = s.detach().cpu().numpy().transpose(0,2,3,1)[0]\n",
    "  s = ((s+1.0)*127.5).clip(0,255).astype(np.uint8)\n",
    "  s = Image.fromarray(s)\n",
    "  display(s)\n",
    "\n",
    "codebook_size = config.model.params.first_stage_config.params.embed_dim\n",
    "z_indices_shape = c_indices.shape\n",
    "z_code_shape = c_code.shape\n",
    "z_indices = torch.randint(codebook_size, z_indices_shape, device=model.device)\n",
    "x_sample = model.decode_to_img(z_indices, z_code_shape)\n",
    "show_image(x_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ftDWneY3zJZ5"
   },
   "source": [
    "## Sample an image\n",
    "\n",
    "We use the transformer in a sliding window manner to sample all code entries sequentially. The code below assumes a window size of $16\\times 16$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 724
    },
    "id": "5rVRrUOwbEH0",
    "outputId": "f5f860e0-6a3c-44ac-9361-bd4d1be8318e"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "idx = z_indices\n",
    "idx = idx.reshape(z_code_shape[0],z_code_shape[2],z_code_shape[3])\n",
    "\n",
    "cidx = c_indices\n",
    "cidx = cidx.reshape(c_code.shape[0],c_code.shape[2],c_code.shape[3])\n",
    "\n",
    "temperature = 1.0\n",
    "top_k = 100\n",
    "update_every = 50\n",
    "\n",
    "start_t = time.time()\n",
    "for i in range(0, z_code_shape[2]-0):\n",
    "  if i <= 8:\n",
    "    local_i = i\n",
    "  elif z_code_shape[2]-i < 8:\n",
    "    local_i = 16-(z_code_shape[2]-i)\n",
    "  else:\n",
    "    local_i = 8\n",
    "  for j in range(0,z_code_shape[3]-0):\n",
    "    if j <= 8:\n",
    "      local_j = j\n",
    "    elif z_code_shape[3]-j < 8:\n",
    "      local_j = 16-(z_code_shape[3]-j)\n",
    "    else:\n",
    "      local_j = 8\n",
    "\n",
    "    i_start = i-local_i\n",
    "    i_end = i_start+16\n",
    "    j_start = j-local_j\n",
    "    j_end = j_start+16\n",
    "    \n",
    "    patch = idx[:,i_start:i_end,j_start:j_end]\n",
    "    patch = patch.reshape(patch.shape[0],-1)\n",
    "    cpatch = cidx[:, i_start:i_end, j_start:j_end]\n",
    "    cpatch = cpatch.reshape(cpatch.shape[0], -1)\n",
    "    patch = torch.cat((cpatch, patch), dim=1)\n",
    "    logits,_ = model.transformer(patch[:,:-1])\n",
    "    logits = logits[:, -256:, :]\n",
    "    logits = logits.reshape(z_code_shape[0],16,16,-1)\n",
    "    logits = logits[:,local_i,local_j,:]\n",
    "\n",
    "    logits = logits/temperature\n",
    "\n",
    "    if top_k is not None:\n",
    "      logits = model.top_k_logits(logits, top_k)\n",
    "\n",
    "    probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    idx[:,i,j] = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "    step = i*z_code_shape[3]+j\n",
    "    if step%update_every==0 or step==z_code_shape[2]*z_code_shape[3]-1:\n",
    "      x_sample = model.decode_to_img(idx, z_code_shape)\n",
    "      clear_output()\n",
    "      print(f\"Time: {time.time() - start_t} seconds\")\n",
    "      print(f\"Step: ({i},{j}) | Local: ({local_i},{local_j}) | Crop: ({i_start}:{i_end},{j_start}:{j_end})\")\n",
    "      show_image(x_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Pu-VjGHnFta"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "taming-transformers.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.10.9 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "e4818a0b8c316263be072c2082609790d2bac6bbfe2378382b84905edb944ba2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
