{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2U0NA9HrrZey"
   },
   "source": [
    "# Taming Transformers\n",
    "\n",
    "This notebook is a minimal working example to generate landscape images as in [Taming Transformers for High-Resolution Image Synthesis](https://github.com/CompVis/taming-transformers). **tl;dr** We combine the efficiancy of convolutional approaches with the expressivity of transformers by introducing a convolutional VQGAN, which learns a codebook of context-rich visual parts, whose composition is modeled with an autoregressive transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4hdc6YonrvoC"
   },
   "source": [
    "## Setup\n",
    "The setup code in this section was written to be [run in a Colab environment](https://colab.research.google.com/github/CompVis/taming-transformers/blob/master/scripts/taming-transformers.ipynb). For a full, local setup, we recommend the provided [conda environment](https://github.com/CompVis/taming-transformers/blob/master/environment.yaml), as [described in the readme](https://github.com/CompVis/taming-transformers#requirements). This will also allow you to run a streamlit based demo.\n",
    "\n",
    "Here, we first clone the repository and download a model checkpoint and config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wwj8j_l201aF",
    "outputId": "833f54a6-0620-4dc1-dd8c-69ab01433d17"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/CompVis/taming-transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd taming-transformers\n",
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-09-13 16:39:56--  https://heibox.uni-heidelberg.de/d/73487ab6e5314cb5adba/files/?p=%2Fcheckpoints%2Flast.ckpt&dl=1\n",
      "Resolving heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)... 129.206.7.113\n",
      "Connecting to heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)|129.206.7.113|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://heibox.uni-heidelberg.de/seafhttp/files/0dc737a6-ccaa-45fe-92d9-b60ac3a142b0/last.ckpt [following]\n",
      "--2023-09-13 16:39:58--  https://heibox.uni-heidelberg.de/seafhttp/files/0dc737a6-ccaa-45fe-92d9-b60ac3a142b0/last.ckpt\n",
      "Reusing existing connection to heibox.uni-heidelberg.de:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4263525412 (4.0G) [application/octet-stream]\n",
      "Saving to: ‘logs/2020-11-09T13-31-51_sflckr/checkpoints/last.ckpt’\n",
      "\n",
      "    logs/2020-11-09   5%[>                   ] 208.53M  8.51MB/s    eta 10m 39s^C\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p logs/2020-11-09T13-31-51_sflckr/checkpoints -p\n",
    "!wget 'https://heibox.uni-heidelberg.de/d/73487ab6e5314cb5adba/files/?p=%2Fcheckpoints%2Flast.ckpt&dl=1' -O 'logs/2020-11-09T13-31-51_sflckr/checkpoints/last.ckpt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "print(os.getcwd())\n",
    "%cd ../\n",
    "!mkdir logs/2020-11-09T13-31-51_sflckr/configs -p\n",
    "!wget 'https://heibox.uni-heidelberg.de/d/73487ab6e5314cb5adba/files/?p=%2Fconfigs%2F2020-11-09T13-31-51-project.yaml&dl=1' -O 'logs/2020-11-09T13-31-51_sflckr/configs/2020-11-09T13-31-51-project.yaml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eeBqWgMQDjZb"
   },
   "source": [
    "Next, we install minimal required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hzQNmIuT_0uF",
    "outputId": "e6e1ce0f-bce9-4ebe-a852-95003ca7b630"
   },
   "outputs": [],
   "source": [
    "%pip install omegaconf>=2.0.0 pytorch-lightning>=1.0.8 einops transformers\n",
    "import sys\n",
    "sys.path.append(\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5gaAQZXTxFxD"
   },
   "source": [
    "## Loading the model\n",
    "\n",
    "We load and print the config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hUOUJaTj02Bq",
    "outputId": "6671562d-f7f6-4407-ee40-22e1b83421e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:\n",
      "  params:\n",
      "    batch_size: 1\n",
      "    validation:\n",
      "      target: taming.data.sflckr.Examples\n",
      "  target: main.DataModuleFromConfig\n",
      "model:\n",
      "  base_learning_rate: 4.5e-06\n",
      "  params:\n",
      "    cond_stage_config:\n",
      "      params:\n",
      "        ddconfig:\n",
      "          attn_resolutions:\n",
      "          - 16\n",
      "          ch: 128\n",
      "          ch_mult:\n",
      "          - 1\n",
      "          - 1\n",
      "          - 2\n",
      "          - 2\n",
      "          - 4\n",
      "          double_z: false\n",
      "          dropout: 0.0\n",
      "          in_channels: 182\n",
      "          num_res_blocks: 2\n",
      "          out_ch: 182\n",
      "          resolution: 256\n",
      "          z_channels: 256\n",
      "        embed_dim: 256\n",
      "        image_key: segmentation\n",
      "        lossconfig:\n",
      "          target: taming.modules.losses.DummyLoss\n",
      "        n_embed: 1024\n",
      "      target: taming.models.vqgan.VQModel\n",
      "    cond_stage_key: segmentation\n",
      "    first_stage_config:\n",
      "      params:\n",
      "        ddconfig:\n",
      "          attn_resolutions:\n",
      "          - 16\n",
      "          ch: 128\n",
      "          ch_mult:\n",
      "          - 1\n",
      "          - 1\n",
      "          - 2\n",
      "          - 2\n",
      "          - 4\n",
      "          double_z: false\n",
      "          dropout: 0.0\n",
      "          in_channels: 3\n",
      "          num_res_blocks: 2\n",
      "          out_ch: 3\n",
      "          resolution: 256\n",
      "          z_channels: 256\n",
      "        embed_dim: 256\n",
      "        lossconfig:\n",
      "          target: taming.modules.losses.DummyLoss\n",
      "        n_embed: 1024\n",
      "      target: taming.models.vqgan.VQModel\n",
      "    first_stage_key: image\n",
      "    transformer_config:\n",
      "      params:\n",
      "        block_size: 512\n",
      "        n_embd: 1024\n",
      "        n_head: 16\n",
      "        n_layer: 24\n",
      "        vocab_size: 1024\n",
      "      target: taming.modules.transformer.mingpt.GPT\n",
      "  target: taming.models.cond_transformer.Net2NetTransformer\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from omegaconf import OmegaConf\n",
    "config_path = \"logs/2020-11-09T13-31-51_sflckr/configs/2020-11-09T13-31-51-project.yaml\"\n",
    "config = OmegaConf.load(config_path)\n",
    "import yaml\n",
    "print(yaml.dump(OmegaConf.to_container(config)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/g/github/ZSE-SBIR/taming-transformers\n",
      "/mnt/g/github/ZSE-SBIR/taming-transformers\n"
     ]
    }
   ],
   "source": [
    "%cd /mnt/g/github/ZSE-SBIR/taming-transformers\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FzQqNgiLEJ9J"
   },
   "source": [
    "Instantiate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sWvDDVoz3RB4",
    "outputId": "08462e1d-77da-4f9d-d3e9-43bdd2be74b8"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'taming'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtaming\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcond_transformer\u001b[39;00m \u001b[39mimport\u001b[39;00m Net2NetTransformer\n\u001b[1;32m      2\u001b[0m model \u001b[39m=\u001b[39m Net2NetTransformer(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mparams)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'taming'"
     ]
    }
   ],
   "source": [
    "from taming.models.cond_transformer import Net2NetTransformer\n",
    "import sys\n",
    "sys.path.append()\n",
    "\n",
    "model = Net2NetTransformer(**config.model.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "njAiY_aqENwV"
   },
   "source": [
    "Load the checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QABLRpVsDhba"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "ckpt_path = \"logs/2020-11-09T13-31-51_sflckr/checkpoints/last.ckpt\"\n",
    "sd = torch.load(ckpt_path, map_location=\"cpu\")[\"state_dict\"]\n",
    "missing, unexpected = model.load_state_dict(sd, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iYN9cREY-r3N",
    "outputId": "0954af07-f4b9-474a-d6bf-45355f2b360d"
   },
   "outputs": [],
   "source": [
    "model.cuda().eval()\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tTusbqk2y0u3"
   },
   "source": [
    "## Load example data\n",
    "\n",
    "Load an example segmentation and visualize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8LiAOU6C-vTP"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "segmentation_path = \"data/sflckr_segmentations/norway/25735082181_999927fe5a_b.png\"\n",
    "segmentation = Image.open(segmentation_path)\n",
    "segmentation = np.array(segmentation)\n",
    "segmentation = np.eye(182)[segmentation]\n",
    "segmentation = torch.tensor(segmentation.transpose(2,0,1)[None]).to(dtype=torch.float32, device=model.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oCNy13FGPMy6"
   },
   "source": [
    "Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 700
    },
    "id": "eZFxCkQ5McKG",
    "outputId": "367a3774-3fcc-405c-c643-4477aad91a2d"
   },
   "outputs": [],
   "source": [
    "def show_segmentation(s):\n",
    "  s = s.detach().cpu().numpy().transpose(0,2,3,1)[0,:,:,None,:]\n",
    "  colorize = np.random.RandomState(1).randn(1,1,s.shape[-1],3)\n",
    "  colorize = colorize / colorize.sum(axis=2, keepdims=True)\n",
    "  s = s@colorize\n",
    "  s = s[...,0,:]\n",
    "  s = ((s+1.0)*127.5).clip(0,255).astype(np.uint8)\n",
    "  s = Image.fromarray(s)\n",
    "  display(s)\n",
    "\n",
    "show_segmentation(segmentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hNBdHGbNTrfM"
   },
   "source": [
    "Our model also employs a VQGAN for the conditioning information, i.e. the segmentation in this example. Let's autoencode the segmentation map. Encoding returns both the quantized code and its representation in terms of indices of a learned codebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 724
    },
    "id": "y_HS2hCCORLr",
    "outputId": "770227a3-533e-4e77-a017-8f6a9b919940"
   },
   "outputs": [],
   "source": [
    "c_code, c_indices = model.encode_to_c(segmentation)\n",
    "print(\"c_code\", c_code.shape, c_code.dtype)\n",
    "print(\"c_indices\", c_indices.shape, c_indices.dtype)\n",
    "assert c_code.shape[2]*c_code.shape[3] == c_indices.shape[0]\n",
    "segmentation_rec = model.cond_stage_model.decode(c_code)\n",
    "show_segmentation(torch.softmax(segmentation_rec, dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pp-OsC1RXAQ9"
   },
   "source": [
    "Let's sample indices corresponding to codes from the image VQGAN given the segmentation code. We init randomly and take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 689
    },
    "id": "VTfao3jJSCfW",
    "outputId": "1a3320ff-a389-4759-ced3-5dee58bf7a29"
   },
   "outputs": [],
   "source": [
    "def show_image(s):\n",
    "  s = s.detach().cpu().numpy().transpose(0,2,3,1)[0]\n",
    "  s = ((s+1.0)*127.5).clip(0,255).astype(np.uint8)\n",
    "  s = Image.fromarray(s)\n",
    "  display(s)\n",
    "\n",
    "codebook_size = config.model.params.first_stage_config.params.embed_dim\n",
    "z_indices_shape = c_indices.shape\n",
    "z_code_shape = c_code.shape\n",
    "z_indices = torch.randint(codebook_size, z_indices_shape, device=model.device)\n",
    "x_sample = model.decode_to_img(z_indices, z_code_shape)\n",
    "show_image(x_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ftDWneY3zJZ5"
   },
   "source": [
    "## Sample an image\n",
    "\n",
    "We use the transformer in a sliding window manner to sample all code entries sequentially. The code below assumes a window size of $16\\times 16$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 724
    },
    "id": "5rVRrUOwbEH0",
    "outputId": "f5f860e0-6a3c-44ac-9361-bd4d1be8318e"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "idx = z_indices\n",
    "idx = idx.reshape(z_code_shape[0],z_code_shape[2],z_code_shape[3])\n",
    "\n",
    "cidx = c_indices\n",
    "cidx = cidx.reshape(c_code.shape[0],c_code.shape[2],c_code.shape[3])\n",
    "\n",
    "temperature = 1.0\n",
    "top_k = 100\n",
    "update_every = 50\n",
    "\n",
    "start_t = time.time()\n",
    "for i in range(0, z_code_shape[2]-0):\n",
    "  if i <= 8:\n",
    "    local_i = i\n",
    "  elif z_code_shape[2]-i < 8:\n",
    "    local_i = 16-(z_code_shape[2]-i)\n",
    "  else:\n",
    "    local_i = 8\n",
    "  for j in range(0,z_code_shape[3]-0):\n",
    "    if j <= 8:\n",
    "      local_j = j\n",
    "    elif z_code_shape[3]-j < 8:\n",
    "      local_j = 16-(z_code_shape[3]-j)\n",
    "    else:\n",
    "      local_j = 8\n",
    "\n",
    "    i_start = i-local_i\n",
    "    i_end = i_start+16\n",
    "    j_start = j-local_j\n",
    "    j_end = j_start+16\n",
    "    \n",
    "    patch = idx[:,i_start:i_end,j_start:j_end]\n",
    "    patch = patch.reshape(patch.shape[0],-1)\n",
    "    cpatch = cidx[:, i_start:i_end, j_start:j_end]\n",
    "    cpatch = cpatch.reshape(cpatch.shape[0], -1)\n",
    "    patch = torch.cat((cpatch, patch), dim=1)\n",
    "    logits,_ = model.transformer(patch[:,:-1])\n",
    "    logits = logits[:, -256:, :]\n",
    "    logits = logits.reshape(z_code_shape[0],16,16,-1)\n",
    "    logits = logits[:,local_i,local_j,:]\n",
    "\n",
    "    logits = logits/temperature\n",
    "\n",
    "    if top_k is not None:\n",
    "      logits = model.top_k_logits(logits, top_k)\n",
    "\n",
    "    probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    idx[:,i,j] = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "    step = i*z_code_shape[3]+j\n",
    "    if step%update_every==0 or step==z_code_shape[2]*z_code_shape[3]-1:\n",
    "      x_sample = model.decode_to_img(idx, z_code_shape)\n",
    "      clear_output()\n",
    "      print(f\"Time: {time.time() - start_t} seconds\")\n",
    "      print(f\"Step: ({i},{j}) | Local: ({local_i},{local_j}) | Crop: ({i_start}:{i_end},{j_start}:{j_end})\")\n",
    "      show_image(x_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Pu-VjGHnFta"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "taming-transformers.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.10.9 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "e4818a0b8c316263be072c2082609790d2bac6bbfe2378382b84905edb944ba2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
