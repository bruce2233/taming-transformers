{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from taming.modules.vqvae.quantize import VectorQuantizer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as TF\n",
    "import torchvision.transforms as T\n",
    "import torch\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "def preprocess(img, target_image_size=256):\n",
    "    s = min(img.size)\n",
    "    \n",
    "    if s < target_image_size:\n",
    "        raise ValueError(f'min dim for image {s} < {target_image_size}')\n",
    "        \n",
    "    r = target_image_size / s\n",
    "    s = (round(r * img.size[1]), round(r * img.size[0]))\n",
    "    img = TF.resize(img, s, interpolation=PIL.Image.LANCZOS)\n",
    "    img = TF.center_crop(img, output_size=2 * [target_image_size])\n",
    "    img = torch.unsqueeze(T.ToTensor()(img), 0)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from taming.modules.diffusionmodules.model import Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"/mnt/g/github/ZSE-SBIR/taming-transformers/logs/vqgan_imagenet_f16_1024/checkpoints/last.ckpt\",map_location=torch.device('cuda:0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "config_path = \"/mnt/g/github/ZSE-SBIR/taming-transformers/logs/vqgan_imagenet_f16_1024/configs/model.yaml\"\n",
    "config = OmegaConf.load(config_path)\n",
    "print(config.model.params.ddconfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_params = {'.'.join(key.split('.')[1:]): value for key,value in model['state_dict'].items() if key.startswith('encoder.')}\n",
    "print(enc_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = Encoder(**config.model.params.ddconfig)\n",
    "\n",
    "enc.load_state_dict(enc_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# im = cv2.imread(\"data/ade20k_segmentations/ADE_val_00000532.png\")\n",
    "# im = cv2.cvtColor(im,cv2.COLOR_BGR2RGB)\n",
    "\n",
    "im = PIL.Image.open(\"/mnt/g/github/ZSE-SBIR/taming-transformers/data/coco_annotations_100/val2017/000000010092.jpg\")\n",
    "print(im.size)\n",
    "\n",
    "im = preprocess(im)\n",
    "# im = einops.rearrange(im.unsqueeze(0), 'b h w c -> b c h w')\n",
    "print(im.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = enc(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantizer = VectorQuantizer2(config.model.params.n_embed, config.model.params.embed_dim, 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k,v in model['state_dict'].items():\n",
    "#     if k.startswith('quantize'):\n",
    "#         print(k)\n",
    "quantizer_params =  {'.'.join(k.split('.')[1:]):v for k,v in model['state_dict'].items() if k.startswith('quantize')}\n",
    "print(quantizer_params)\n",
    "quantizer.load_state_dict(quantizer_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = quantizer(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': {'base_learning_rate': 4.5e-06, 'target': 'taming.models.cond_transformer.Net2NetTransformer', 'params': {'cond_stage_key': 'depth', 'transformer_config': {'target': 'taming.modules.transformer.mingpt.GPT', 'params': {'vocab_size': 1024, 'block_size': 512, 'n_layer': 24, 'n_head': 16, 'n_embd': 1024}}, 'first_stage_config': {'target': 'taming.models.vqgan.VQModel', 'params': {'ckpt_path': 'logs/2020-09-23T17-56-33_imagenet_vqgan/checkpoints/last.ckpt', 'embed_dim': 256, 'n_embed': 1024, 'ddconfig': {'double_z': False, 'z_channels': 256, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 1, 2, 2, 4], 'num_res_blocks': 2, 'attn_resolutions': [16], 'dropout': 0.0}, 'lossconfig': {'target': 'taming.modules.losses.DummyLoss'}}}, 'cond_stage_config': {'target': 'taming.models.vqgan.VQModel', 'params': {'ckpt_path': 'logs/2020-11-03T15-34-24_imagenetdepth_vqgan/checkpoints/last.ckpt', 'embed_dim': 256, 'n_embed': 1024, 'ddconfig': {'double_z': False, 'z_channels': 256, 'resolution': 256, 'in_channels': 1, 'out_ch': 1, 'ch': 128, 'ch_mult': [1, 1, 2, 2, 4], 'num_res_blocks': 2, 'attn_resolutions': [16], 'dropout': 0.0}, 'lossconfig': {'target': 'taming.modules.losses.DummyLoss'}}}}}, 'data': {'target': 'main.DataModuleFromConfig', 'params': {'batch_size': 2, 'num_workers': 8, 'train': {'target': 'taming.data.imagenet.RINTrainWithDepth', 'params': {'size': 256}}, 'validation': {'target': 'taming.data.imagenet.RINValidationWithDepth', 'params': {'size': 256}}}}}\n",
      "{'target': 'taming.modules.transformer.mingpt.GPT', 'params': {'vocab_size': 1024, 'block_size': 512, 'n_layer': 24, 'n_head': 16, 'n_embd': 1024}}\n"
     ]
    }
   ],
   "source": [
    "depth_config = config = OmegaConf.load(\"/mnt/g/github/ZSE-SBIR/taming-transformers/logs/2020-11-20T12-54-32_drin_transformer/configs/2020-11-20T12-54-32-project.yaml\")\n",
    "print(depth_config)\n",
    "print(depth_config.model.params.transformer_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from taming.models.cond_transformer import Net2NetTransformer\n",
    "transformer = Net2NetTransformer(depth_config.model.params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e4818a0b8c316263be072c2082609790d2bac6bbfe2378382b84905edb944ba2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
